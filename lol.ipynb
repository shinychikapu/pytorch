{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle= True, drop_last= True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#X is 64 pictures, 1 channel, 28X28\n",
    "for picture, label in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {picture.shape}\")\n",
    "    print(f\"Shape of y: {label.shape} {label.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing the Model Parameters**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model, we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the modelâ€™s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() #compute the loss function's gradients\n",
    "        optimizer.step() #update the parameters\n",
    "        optimizer.zero_grad() #clear the model's gradient to avoid gradient accumulation\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1] #first 2 pictures in the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0510, 0.2627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.1961, 0.1490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314,\n",
       "          0.4706, 0.8196, 0.8863, 0.9686, 0.9294, 1.0000, 1.0000, 1.0000,\n",
       "          0.9686, 0.9333, 0.9216, 0.6745, 0.2824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5373, 0.9373,\n",
       "          0.9882, 0.9529, 0.9176, 0.8980, 0.9333, 0.9569, 0.9647, 0.9412,\n",
       "          0.9020, 0.9098, 0.9373, 0.9725, 0.9843, 0.7608, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 1.0000, 0.9059,\n",
       "          0.8941, 0.8902, 0.8941, 0.9137, 0.9020, 0.9020, 0.8980, 0.8941,\n",
       "          0.9098, 0.9098, 0.9059, 0.8902, 0.8784, 0.9882, 0.7020, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9137, 0.9451, 0.8980,\n",
       "          0.9059, 1.0000, 1.0000, 0.9333, 0.9059, 0.8902, 0.9333, 0.9647,\n",
       "          0.8941, 0.9020, 0.8902, 0.9176, 0.9216, 0.8980, 0.9451, 0.0784,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9725, 0.9451, 0.9059,\n",
       "          1.0000, 0.5843, 0.1843, 0.9882, 0.8941, 1.0000, 0.9490, 0.8471,\n",
       "          0.9333, 0.9098, 1.0000, 0.8941, 0.8627, 0.9176, 0.9804, 0.2118,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9412, 0.9098,\n",
       "          1.0000, 0.0588, 0.0000, 1.0000, 0.9294, 0.7490, 0.0000, 0.0000,\n",
       "          0.8392, 1.0000, 0.0510, 0.4824, 1.0000, 0.9176, 0.9882, 0.4471,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 1.0000, 0.9333, 0.9373,\n",
       "          1.0000, 0.6941, 0.0000, 1.0000, 1.0000, 0.0000, 0.5098, 0.4549,\n",
       "          0.1843, 0.2549, 0.1686, 0.1451, 1.0000, 0.9255, 0.9765, 0.6353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1255, 1.0000, 0.9255, 0.9608,\n",
       "          1.0000, 0.8000, 0.0000, 1.0000, 0.3294, 0.0000, 0.1451, 0.1098,\n",
       "          0.1216, 0.0000, 0.0980, 0.0510, 1.0000, 0.9255, 0.9765, 0.7804,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2078, 1.0000, 0.9255, 0.9804,\n",
       "          0.9804, 0.9059, 0.0078, 1.0000, 0.0824, 0.0000, 0.8667, 1.0000,\n",
       "          0.9255, 0.2118, 0.9608, 0.7765, 0.9529, 0.9333, 0.9608, 0.8745,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 1.0000, 0.9294, 0.9804,\n",
       "          0.9412, 1.0000, 0.0000, 0.0000, 0.1529, 0.6157, 0.0000, 0.0000,\n",
       "          0.8431, 0.3686, 0.0784, 0.4941, 1.0000, 0.9294, 0.9373, 0.9804,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.3961, 1.0000, 0.9216, 0.9922,\n",
       "          0.9569, 0.9529, 0.5216, 0.5412, 0.8157, 1.0000, 0.7882, 0.8392,\n",
       "          1.0000, 0.9020, 0.0275, 0.6824, 1.0000, 0.9412, 0.9333, 1.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 1.0000, 0.9137, 1.0000,\n",
       "          0.9725, 0.9137, 1.0000, 1.0000, 0.9412, 0.9098, 0.9529, 0.9529,\n",
       "          0.9059, 0.9843, 1.0000, 1.0000, 0.9961, 0.9529, 0.9333, 1.0000,\n",
       "          0.0118, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5765, 1.0000, 0.9137, 0.9765,\n",
       "          0.7098, 0.9529, 0.8902, 0.8784, 0.9020, 0.9176, 0.9020, 0.9020,\n",
       "          0.9216, 0.8941, 0.9216, 0.8706, 0.8118, 1.0000, 0.9255, 1.0000,\n",
       "          0.1373, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.6392, 1.0000, 0.9608, 0.8667,\n",
       "          0.3373, 1.0000, 0.9137, 0.9137, 0.9216, 0.9255, 0.9176, 0.9176,\n",
       "          0.9176, 0.9098, 0.9490, 0.9059, 0.4902, 1.0000, 0.9255, 1.0000,\n",
       "          0.2157, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7098, 0.9961, 1.0000, 0.7843,\n",
       "          0.2706, 1.0000, 0.8941, 0.9098, 0.9176, 0.9216, 0.9176, 0.9176,\n",
       "          0.9137, 0.9216, 0.9451, 0.9294, 0.2745, 1.0000, 0.9216, 0.9647,\n",
       "          0.2235, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7725, 0.9686, 1.0000, 0.7373,\n",
       "          0.4314, 1.0000, 0.8784, 0.9137, 0.9176, 0.9176, 0.9176, 0.9176,\n",
       "          0.9176, 0.9176, 0.9412, 0.9922, 0.2706, 1.0000, 0.9255, 0.9725,\n",
       "          0.3020, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.9647, 1.0000, 0.5843,\n",
       "          0.5686, 1.0000, 0.8745, 0.9216, 0.9176, 0.9216, 0.9216, 0.9216,\n",
       "          0.9176, 0.9294, 0.9137, 1.0000, 0.1843, 1.0000, 0.9373, 0.9765,\n",
       "          0.3843, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.9529, 1.0000, 0.4353,\n",
       "          0.6784, 1.0000, 0.8902, 0.9216, 0.9216, 0.9255, 0.9216, 0.9216,\n",
       "          0.9216, 0.9373, 0.8980, 1.0000, 0.0745, 0.8902, 0.9647, 0.9765,\n",
       "          0.4314, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7686, 0.9412, 1.0000, 0.4275,\n",
       "          0.8353, 0.9804, 0.8980, 0.9216, 0.9216, 0.9255, 0.9216, 0.9294,\n",
       "          0.9255, 0.9294, 0.8863, 1.0000, 0.2157, 0.7961, 0.9843, 0.9608,\n",
       "          0.4706, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7529, 0.9529, 1.0000, 0.4471,\n",
       "          0.9098, 0.9412, 0.9098, 0.9216, 0.9216, 0.9255, 0.9176, 0.9294,\n",
       "          0.9255, 0.9216, 0.8980, 1.0000, 0.5255, 0.6706, 0.9882, 0.9569,\n",
       "          0.5373, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7412, 0.9843, 1.0000, 0.6039,\n",
       "          0.9333, 0.9137, 0.9255, 0.9176, 0.9216, 0.9255, 0.9216, 0.9333,\n",
       "          0.9255, 0.9216, 0.9098, 1.0000, 0.6510, 0.4902, 1.0000, 0.9529,\n",
       "          0.5569, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.9882, 1.0000, 0.6706,\n",
       "          0.9686, 0.9098, 0.9176, 0.9176, 0.9137, 0.9137, 0.9098, 0.9176,\n",
       "          0.9137, 0.9176, 0.9137, 0.9412, 0.8745, 0.5020, 1.0000, 0.9490,\n",
       "          0.5922, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9529, 1.0000, 0.2235,\n",
       "          0.9333, 0.9451, 0.9333, 0.9333, 0.9333, 0.9294, 0.9255, 0.9294,\n",
       "          0.9294, 0.9412, 0.9294, 0.9961, 0.6902, 0.2039, 1.0000, 0.9373,\n",
       "          0.6157, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.9412, 0.9804, 0.2431,\n",
       "          0.8549, 1.0000, 0.8627, 0.8706, 0.8706, 0.8706, 0.8745, 0.8745,\n",
       "          0.8784, 0.8706, 0.8549, 1.0000, 0.6039, 0.1255, 1.0000, 0.9255,\n",
       "          0.7373, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5098, 0.9608, 0.9490, 0.0941,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333, 0.9490, 0.9569,\n",
       "          0.5294, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2980, 1.0000, 0.9765, 0.0863,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.9765, 1.0000,\n",
       "          0.4824, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.8039, 0.7725, 0.0431,\n",
       "          0.0000, 0.0157, 0.0039, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
       "          0.0078, 0.0078, 0.0078, 0.0118, 0.0000, 0.0118, 0.6824, 0.7412,\n",
       "          0.2627, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1][0] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Showing a picture in the data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2x2 matrix where the first column is the tensor of the picture and the second column is the label of the picture\n",
    "#reshape to match opencv (w, h, c)\n",
    "pic = test_data[1][0].reshape(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale color from 0-1 to 0-255\n",
    "pic = np.uint8(pic * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<1,-1,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#convert to gray scale\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_bgr \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mcvtColor(pic, cv\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m      3\u001b[0m image_bgr\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<1,-1,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "#convert to gray scale\n",
    "image_bgr = cv.cvtColor(pic, cv2.COLOR_BGR2GRAY)\n",
    "image_bgr[:, : , 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the grayscale image to (28, 28, 1)\n",
    "gray_image = image_bgr[:, :, np.newaxis]\n",
    "gray_image.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the cv2.resize() function to upscale the image. The resize() function takes the following arguments:\n",
    "\n",
    "The image to be resized (image in this case).\n",
    "The target size, specified by None in this example, as we want to specify the scale factors instead.\n",
    "\n",
    "The scaling factors fx and fy, which determine the amount of upscaling. Here, we've set both factors to 2 to double the size of the image.\n",
    "\n",
    "The interpolation method, which determines how the new pixels are generated. In this case, we've used cv2.INTER_CUBIC, which provides a higher-quality upscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled_image = cv.resize(pic, None, fx=2, fy=2, interpolation=cv.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imshow(\"photo\", upscaled_image)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn a photo up side down and feed it to the model XDDDD**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Turning an image up side down*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pic = test_data[5][0].reshape(28,28,1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsidedown = cv.flip(random_pic, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsidedown.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Convert it back to tensor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsidedown = torch.from_numpy(upsidedown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsidedown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_label = test_data[5][1]\n",
    "pic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the image\n",
    "upsidedown = upsidedown.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsidedown.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning an image 90 degrees and feed it to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "piccc = test_data[28][0].reshape(28, 28, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the image 90 degrees clockwise using rotate() function\n",
    "rotated_image = cv.rotate(piccc, cv.ROTATE_90_CLOCKWISE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_image = rotated_image.reshape(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_image = torch.from_numpy(rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_label = test_data[28][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,  18,  18, ..., 245, 245, 246],\n",
       "       [ 18,  18,  18, ..., 245, 245, 246],\n",
       "       [ 18,  18,  18, ..., 245, 245, 246],\n",
       "       ...,\n",
       "       [ 16,  16,  16, ..., 136, 136, 134],\n",
       "       [ 16,  16,  16, ..., 133, 133, 133],\n",
       "       [ 16,  16,  16, ..., 125, 125, 125]], dtype=uint8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testImg = cv.imread(\"seventythree.PNG\")\n",
    "testImg = cv.cvtColor(testImg, cv.COLOR_BGR2GRAY)\n",
    "testImg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A function that take in any image and resize it to 28, 28 gray scale to pass into the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat(img_path):\n",
    "    '''This function take in a image path and return it as a (1, 28, 28) tensor to pass into the NN Model\n",
    "    \n",
    "    Attribute:\n",
    "    ---------\n",
    "    img_path : the path to the image on the computer\n",
    "    '''\n",
    "\n",
    "    img = cv.imread(img_path)\n",
    "    \n",
    "\n",
    "    #displaying the image\n",
    "    cv.imshow(\"img\", img)\n",
    "    cv.waitKey(0) \n",
    "    \n",
    "    if (img.shape == (28, 28, 1)):\n",
    "        return torch.from_numpy(img).reshape(1, 28, 28)\n",
    "\n",
    "    #resize to 28x28\n",
    "    img = cv.resize(img, (28, 28)) \n",
    "\n",
    "    #convert to gray scale\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) #note that color is still from 0 to 255\n",
    "\n",
    "\n",
    "    #Making the color to be between 0 and 255\n",
    "    img = np.float32(img / 255)\n",
    "\n",
    "    #reshape and convert to tensor\n",
    "    img_ = torch.from_numpy(img).reshape(1, 28, 28)\n",
    "    return img_\n",
    "\n",
    "\n",
    "testtt = reformat(\"seventythree.PNG\")\n",
    "testtt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'with torch.no_grad' : disables gradient calculation during the forward pass to reduce memory usage and speed up computation when we don't need to compute gradients. It's typically used during inference or evaluation when we don't need to update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Shirt\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    rotated_image = rotated_image.to(device)# This line of code moves the input tensor x to the specified device (e.g., CPU or GPU) for computation. \n",
    "                    #The to() method is used to transfer tensors between devices.\n",
    "    pred = model(rotated_image) # a 1 x 10 row vector so have to use index 0 to access the row. \n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[rotate_label] #The element with the highest value is the resulting class\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
